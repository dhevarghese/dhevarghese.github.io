<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ClipCap Evolved | Dheeraj Varghese </title> <meta name="author" content="Dheeraj Varghese"> <meta name="description" content="Bridging the Gap Between Modalities"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BE%E2%80%8D%F0%9F%94%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dhevarghese.github.io/blog/2023/clipcap-evolved/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "ClipCap Evolved",
            "description": "Bridging the Gap Between Modalities",
            "published": "March 28, 2023",
            "authors": [
              
              {
                "author": "Dheeraj Varghese",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Vrije Universiteit",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Dawid Kopiczko",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Vrije Universiteit",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Abishek Thamma",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Vrije Universiteit",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Priyakshi Goswami",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Vrije Universiteit",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Tom Pelletreau-Duris",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Vrije Universiteit",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dheeraj</span> Varghese </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">code </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>ClipCap Evolved</h1> <p>Bridging the Gap Between Modalities</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introducing-clipcap">Introducing ClipCap</a> </div> <div> <a href="#background">Background</a> </div> <ul> <li> <a href="#clip-vit">CLIP-ViT</a> </li> <li> <a href="#gpt-2">GPT-2</a> </li> <li> <a href="#flan-t5">FLAN-T5</a> </li> <li> <a href="#parameter-efficient-fine-tuning">Parameter Efficient Fine Tuning</a> </li> </ul> <div> <a href="#approaches">Approaches</a> </div> <ul> <li> <a href="#baselines">Baselines</a> </li> <li> <a href="#language-model-architecture-matters">Language Model Architecture Matters</a> </li> <li> <a href="#beyond-pooled-features">Beyond Pooled Features</a> </li> <li> <a href="#peft-with-lora">PEFT with LoRA</a> </li> <li> <a href="#configuration-of-mappers">Configuration of Mappers</a> </li> </ul> <div> <a href="#methodology">Methodology</a> </div> <ul> <li> <a href="#datasets">Datasets</a> </li> <li> <a href="#evaluation">Evaluation</a> </li> </ul> <div> <a href="#results">Results</a> </div> <ul> <li> <a href="#baseline-runs">Baseline Runs</a> </li> <li> <a href="#comparison-of-lms">Comparison of LMs</a> </li> <li> <a href="#analyzing-the-utility-of-unpooled-visual-representations">Analyzing the Utility of Unpooled Visual Representations</a> </li> <li> <a href="#application-of-lora">Application of LoRA</a> </li> <li> <a href="#t5-weights">T5 Weights</a> </li> <li> <a href="#nocaps-results">Nocaps results</a> </li> <li> <a href="#qualitative-results">Qualitative Results</a> </li> </ul> <div> <a href="#discussion">Discussion</a> </div> <div> <a href="#further-work">Further Work</a> </div> </nav> </d-contents> <p>Image captioning is a challenging vision-language task that involves automatically generating relevant and valid captions that describes the content of an image.</p> <p>Over the years, various approaches have been proposed to tackle this task, with different architectures and training methods being employed. Traditionally, most image captioning pipelines rely on a combination of a visual encoder to encode visual information and a textual decoder that generates captions based on the encoded features. Earlier deep learning based image captioning approaches typically used CNN-encoders and RNN-language models (<a href="https://arxiv.org/abs/1412.2306" rel="external nofollow noopener" target="_blank">Karpathy et al.</a>, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf" rel="external nofollow noopener" target="_blank">Vinyals et al.</a>), however with recent trends <a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Transformer</a>-based models have gained popularity. The visual feature extraction stage has also seen significant changes, moving towards the use of multi-modal architectures trained on large-scale data with language supervision, as seen in models like <a href="https://arxiv.org/abs/2103.00020" rel="external nofollow noopener" target="_blank">CLIP (Contrastive Language-Image Pretraining)</a>.</p> <p>One prevalent approach to image captioning involves utilizing pretrained vision and language models, which are then fine-tuned. Our baseline approach, called <a href="https://arxiv.org/abs/2111.09734" rel="external nofollow noopener" target="_blank">ClipCap</a>, adheres to this paradigm by employing CLIP-ViT as the visual encoder and GPT-2 as the textual decoder. In their approach, the image embeddings are sent as prefixes of captions to the Language Model (LM) which then generates the next token. Alternative approaches like <a href="https://arxiv.org/abs/2204.14198" rel="external nofollow noopener" target="_blank">Flamingo</a> and <a href="https://arxiv.org/abs/2201.12723" rel="external nofollow noopener" target="_blank">VC-GPT</a> fuse visual information from the encoder directly into the layers of a pre-trained LM using a cross attention mechanism.</p> <p>In this blog post, we share our work building upon ClipCap and address key research questions. We review the background and key components, including models and fine-tuning techniques. Our proposed methods are presented, highlighting improvements over the baseline. We also discuss our experiments, results, and future directions.</p> <p><em>The “Background” section offers a brief overview of CLIP, language models, and parameter-efficient fine-tuning methods. It aims to familiarize readers with these essential aspects before diving into the main part. If you are already familiar with these concepts, feel free to skip this section and proceed further.</em></p> <h2 id="introducing-clipcap"><strong>Introducing ClipCap</strong></h2> <p>The authors of ClipCap propose a simple yet effective technique to generate captions. As mentioned before, CLIP is utilised to extract the visual embeddings of the image, which is the condensed representation of the content. This is used as a prefix to the GPT2 input, which then generates the caption based on both the image and the prefix. A simple mapping network is employed to transform the embedding into a compatible format for GPT2. They follow two approaches,</p> <ol> <li>Using an MLP mapper, along with finetuning GPT2</li> <li>Using a transformer mapper</li> </ol> <p>Their second approach demonstrate that training the mapping network alone can yield competent captioning results while keeping CLIP and the LM frozen.</p> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/Clipcap.png" alt="Clipcap"></p> <p>At the time of their publication, this method achieved comparable performance to State-Of-The-Art approaches on challenging datasets such as Conceptual Captions and nocaps, while being simpler, faster, and lighter. However, it is worth noting a couple of potential weaknesses. Firstly, they failed to explore the utility of unpooled visual representations, which may affect its ability to capture fine-grained visual details; and the limited evaluation with different language models, which may leave room for further exploration and analysis. This is exactly what inspired us to explore and pursue this research direction.</p> <h2 id="background"><strong>Background</strong></h2> <p>In this section, we introduce the essential models and methods that serve as building blocks for our image captioning architectures.</p> <h3 id="clip-vit">CLIP-ViT</h3> <p>Contrastive Language Pre-Training (CLIP) is an efficient method of learning from natural language supervision developed by OpenAI. Designed to understand and generate meaningful associations between text and images, CLIP models are effective multimodal visual language models that can be used for a range of tasks, including zero-shot image classification and image-text similarity.</p> <h4 id="architecture">Architecture</h4> <p>CLIP architecture consists of two main components, a text encoder, and an image encoder. These two encoders are jointly trained using a contrastive learning approach to predict the correct pairings of a batch of training (image, text) examples. The CLIP model encodes textual and visual information into a multimodal embedding space, with an aim to increase the cosine similarity score of images and text representations.</p> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/architecture_1.jpeg" alt="architecture_1"></p> <p>The original clip implementation uses a transformer as its text encoder. For the image encoder, the authors propose two separate architectures, one with a <a href="https://arxiv.org/abs/1512.03385" rel="external nofollow noopener" target="_blank">ResNet</a>, and the other with a <a href="https://arxiv.org/abs/2010.11929" rel="external nofollow noopener" target="_blank">Vision Transformer (ViT)</a>.</p> <h3 id="vision-transformer">Vision Transformer</h3> <p>The Vision Transformer (ViT) model architecture was introduced in the paper titled “An Image is Worth 16*16 Words: Transformers for Image Recognition at Scale”, where the authors utilise the transformer architecture for image processing tasks. The proposed architecture involves processing images by splitting an image into fixed size patches, linearly embedding them along with positional embeddings, and then inputting the resultant sequence of vectors to a standard transformer architecture.</p> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/architecture_2.jpeg" alt="architecture_2"></p> <p>The results of the experiments demonstrate that the ViT encoder architecture performs better than the ResNet based encoder architecture on a wide range of datasets. Additionally, the baseline ClipCap implementation uses the CLIP-ViT as its image encoder.</p> <p>In case of CLIP-ViT, the output tokens from the Vision Transformer are pooled into a single vector and passed through a projecting linear layer.</p> <h3 id="gpt-2">GPT-2</h3> <p>OpenAI’s <a href="https://openai.com/research/gpt-2-1-5b-release" rel="external nofollow noopener" target="_blank">GPT-2 (Generative Pretrained Transformer 2)</a> is a large transformer-based language model pretrained on an extensive corpus of English text in a self-supervised manner, enabling it to learn a comprehensive understanding of language and generate coherent and contextually relevant text.</p> <p>GPT-2 is pretrained in a self-supervised way on raw data without any human labelling with an automatic process to generate inputs and labels from those texts. More specifically, the model is trained to predict the next token in sentences. The inputs to the model are sequences of continuous text, with a specific length, and the targets are the same sequences, but shifted one token to the right. The model internally employs a masked self-attention mechanism, ensuring that predictions for a given token only use the inputs up to that token and not the future tokens. This autoregressive training setup enables the model to capture the sequential dependencies and learn the underlying patterns in the language.</p> <p>There are several sizes of GPT-2 available:</p> <table> <thead> <tr> <th>GPT-2 variant</th> <th>Small</th> <th>Medium</th> <th>Large</th> <th>Extra Large</th> </tr> </thead> <tbody> <tr> <td>Parameters</td> <td>117M</td> <td>345M</td> <td>762M</td> <td>1,542M</td> </tr> </tbody> </table> <h3 id="flan-t5">FLAN-T5</h3> <p><a href="https://arxiv.org/abs/2210.11416" rel="external nofollow noopener" target="_blank">Flan-T5</a> is an enhanced version of the original <a href="https://arxiv.org/abs/1910.10683" rel="external nofollow noopener" target="_blank">T5 architecture</a> intoduced by Google in 2019. The T5 (Text-to-Text Transfer Transformer) architecture is built on standard encoder-decoder transformer. The encoder processes the input text, generating a representation that captures contextual information and semantic understanding. This representation serves as a conditioning signal for the decoder. The decoder, in turn, attends to this representation, which uses it to generate the output text gradually.</p> <p>T5 follows a “text-to-text” approach, where all NLP tasks are framed as text-to-text problems. This allows T5 to be fine-tuned on various downstream tasks with minimal modifications, making it highly versatile. FLAN-T5 (Fine-tuned Language Net) improves upon T5 by fine-tuning it on a diverse set of tasks that cover various languages and domains. This enables FLAN-T5 to achieve state-of-the-art performance on several benchmarks.</p> <p>Flan-T5 model offers 5 different variants:</p> <table> <thead> <tr> <th>FLAN-T5 variant</th> <th>Small</th> <th>Base</th> <th>Large</th> <th>XL</th> <th>XXL</th> </tr> </thead> <tbody> <tr> <td>Parameters</td> <td>80M</td> <td>250M</td> <td>780M</td> <td>3B</td> <td>11B</td> </tr> </tbody> </table> <h3 id="parameter-efficient-fine-tuning">Parameter Efficient Fine Tuning</h3> <p>A common practice in the field of NLP is the usage of pretrained models and adapting it to other downstream tasks by finetuning it for a particular task or dataset. However, as LLMs are becoming increasingly large with billions of parameters, it becomes prohibitively hard to train such models. Parameter-Efficient Fine-Tuning (PEFT) techniques help overcome this by freezing most of the pretrained model’s parameters and modifying a small subset of the parameters. <a href="https://arxiv.org/abs/2203.06904" rel="external nofollow noopener" target="_blank">“Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Model”</a> broadly classifies these techniques into 3 categories:</p> <ul> <li>Addition-based methods, which add new parameters to the original model and fine-tune them while keeping the original parameters fixed. Examples include <a href="http://proceedings.mlr.press/v97/houlsby19a.html" rel="external nofollow noopener" target="_blank">Adapters-based tuning</a> and <a href="https://arxiv.org/abs/2108.02035" rel="external nofollow noopener" target="_blank">prompt based tuning</a>.</li> <li>Specification-based methods only fine-tune a subset of the original parameters that are specified by <a href="https://arxiv.org/abs/1911.03090" rel="external nofollow noopener" target="_blank">heuristics</a>.</li> <li>Reparameterization-based methods transform the adaptive parameters during optimization into parameter efficient forms, typically motivated by the hypothesis that LM adaptations towards most downstream tasks are inherently low-rank, and could thus be equivalently completed in a parameter-efficient way. A particularly interesting implementation is that of <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">Low-Rank Adaptation (LoRA)</a> of large language models.</li> </ul> <h4 id="low-rank-adaptation">Low Rank Adaptation</h4> <p>Low Rank Adaptation (LoRA) is a technique introduced by Hu et al. in their paper as efficient fine tuning technique that can greatly reduce the number of trainable parameters for downstream tasks, by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. In particular, LoRA use tensor-train decomposition of the weight matrix and decompose it into two “update matrices”.</p> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/lora.png" alt="lora"></p> <p>For any model layer that can be expressed as a matrix multiplication of the form $h=W_0x$, it can be reparametrised as follows</p> <p>$h = W_0+\frac{\alpha}{r}BAx$</p> <p>where, $A\in\mathbb{R}^{r \times k}$ and $B\in\mathbb{R}^{d \times r}$ and <em>r</em> is the low dimensional rank of the decomposition.</p> <h5 id="advantages-of-lora">Advantages of LoRA</h5> <ul> <li>Since the original pretrained language model is frozen, training using LoRA is more efficient, as we do not need to calculate the gradients or maintain optimizer states for most parameters, and calculate only for the injected much smaller lower rank matrices. The authors note that for a large Transformer trained with Adam, we reduce that VRAM usage by up to 2/3 if r «  $d_{model}$ as we do not need to store the optimizer states for the frozen parameters</li> <li>Adapter layers often introduce inference latency, by extending model depth or reducing the model’s usable sequence length. LoRA overcomes this issue by proposing that when deploying models in production we can explicitly compute $W = W_0+BA$ and store the weight matrix and perform inference as usual. When the pretrained LM needs to be adapted to a new downstream task, the original $W_0$ can be recovered by subtracting BA and a new B’A’ can be summed for the new task</li> <li>LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning.</li> </ul> <h2 id="approaches"><strong>Approaches</strong></h2> <p>In this study, we explore different approaches to improve the generation of accurate and descriptive captions for images, while considering the impact on trainable parameters.</p> <h3 id="baselines">Baselines</h3> <p>We begin by replicating the ClipCap architectures to establish baseline performance. Our focus lies on replicating two variants of the architecture, while employing the CLIP-VIT/32 model as the visual encoder. These serve as reference points to evaluate and compare the effectiveness of our approaches.</p> <h5 id="clipcap-the-mlp-approach">ClipCap, The MLP Approach</h5> <p>       <img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/baseline_1.png" alt="baseline_1"></p> <h5 id="clipcap-the-transformer-approach">ClipCap, The Transformer Approach</h5> <p>       <img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/Baseline_2.png" alt="baseline_2"></p> <h3 id="language-model-architecture-matters">Language Model Architecture Matters</h3> <p>ClipCap utilized GPT2, a decoder-only transformer, to generate tokens autoregressively. While this approach proved effective, we sought to optimize it further by introducing an additional conditioning signal to the cross-attention layers. This insight was inspired by techniques used in the Flamingo paper. The addition of a conditioning signal to the decoder blocks is hypothesized to enhance caption generation performance. This signal delivers an additional layer of context at each decoding step, thus facilitating the decoder to construct more accurate and coherent output.</p> <p>Based on this observation, we explore the use of encoder-decoder models as a promising direction. This resulted in our incorporation of the Flan-T5 model into the ClipCap architecture. The decision to integrate Flan-T5 into ClipCap was motivated by its versatility in handling a multitude of tasks, each one encoded by the encoder. This presents a unique opportunity for improving the caption prediction process. By feeding a prefixed sentence to the encoder block, we are priming the decoder, theoretically enabling it to predict captions more effectively. This is predicated on the hypothesis that the encoder’s capacity to embed different tasks will substantially enhance the decoder’s proficiency in generating precise and pertinent captions.</p> <h4 id="using-flan-t5-as-the-lm">Using FLAN-T5 as the LM</h4> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/Flant5_1.png" alt="CLIP-VIT/32 + MLP + FLAN-T5"></p> <h4 id="using-flan-t5-decoder-only">Using FLAN-T5 Decoder Only</h4> <p>Another approach in our exploration involves utilising only the decoder component of the FLAN-T5 model. In this variant, we decided to bypass the encoder and feed the inputs from the previous components directly to the pre-trained cross attention layers of the decoder. We tested this variant with the two mappers: MLP and Transformer.</p> <h5 id="flan-t5-decoder-the-mlp-approach">FLAN-T5 Decoder, The MLP Approach</h5> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/flant5_2.png" alt="CLIP-VIT/32 + MLP + FLAN-T5 Decoder Only"></p> <h5 id="flan-t5-decoder-the-transformer-approach">FLAN-T5 Decoder, The Transformer Approach</h5> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/flant5_2.png" alt="CLIP-VIT/32 + Transformer + FLAN-T5 Decoder Only"></p> <h3 id="beyond-pooled-features">Beyond Pooled Features</h3> <p>In order to enhance the utilization of visual representations in our models, we propose a departure from using pooled and projected features. Instead, we advocate for leveraging the unpooled representations, which capture more comprehensive visual information. By preserving the richness of visual details that can be lost through pooling and projection, we aim to provide the language model with a more robust and nuanced image representation.</p> <p>To effectively incorporate these unpooled visual tokens into our models, we take steps to align the representation spaces of the visual and language models. This involves passing the visual tokens through a Multilayer Perceptron with shared weights for all tokens. Subsequently, these refined tokens are fed into the language model. For GPT2 and Flan-T5, they act as the prefix, while for the Flan-T5 decoder, they serve as the entire conditioning signal. We anticipate that this tweak will result in improved performance.</p> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/unpooled.png" alt="CLIP-Projections"></p> <p>The shared MLP projects the visual tokens that are not pooled. This means we utilise all the tokens that CLIP-ViT outputs. These tokens, after projection, are directly mapped to the LM.</p> <h3 id="peft-with-lora">PEFT with LoRA</h3> <p>Translating between the representations of image encoder of CLIP and the language model was a challenge faced by the authors of ClipCap. This is owed to the independent training of both models, leading to separate latent spaces. To address this, the authors emphasize the need for fine-tuning the Language Model (LM) along with the mapping network training. However, it is to be noted that fine-tuning the LM substantially escalates the number of trainable parameters (~156M for GPT2). As an alternative approach, the authors freeze the LM and replace the MLP mapping network with a transformer, effectively reducing the trainable parameters to ~43M.</p> <p>To further optimize the model, we experiment with LoRA, a parameter efficient fine tuning technique. We apply LoRA to the baseline architecture (MLP mapper + GPT2) and our best-performing models. We also test it across all layers as well as a subset of layers of the LM.</p> <h3 id="configuration-of-mappers">Configuration of Mappers</h3> <p>For all architectures utilising the MLP mapper we use the following hyperparameters:</p> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Hidden Layers</td> <td>1</td> </tr> <tr> <td>Hidden Layer Size</td> <td>3840</td> </tr> <tr> <td>Activation</td> <td>Tanh</td> </tr> <tr> <td>LM Prefix Length</td> <td>10</td> </tr> </tbody> </table> <p>The configuration of the transformer mapper:</p> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Num Layers</td> <td>8</td> </tr> <tr> <td>Attention Heads</td> <td>8</td> </tr> <tr> <td>Embedding Dimension</td> <td>768</td> </tr> <tr> <td>Trainable Prefix Length</td> <td>10</td> </tr> <tr> <td>LM Prefix Length</td> <td>10</td> </tr> </tbody> </table> <h2 id="methodology"><strong>Methodology</strong></h2> <p>We used CLIP-ViT/32, GPT-2, and FLANT5 models sourced from <a href="https://huggingface.co/docs/transformers/index" rel="external nofollow noopener" target="_blank">Hugging Face’s Transformers library</a>. In order to maintain consistency with the original ClipCap approach, we have preserved all hyperparameters:</p> <ul> <li>Batch size: 40</li> <li>Learning rate: 2e-5</li> <li>Optimizer: AdamW</li> <li>Warm-up steps: 5000</li> </ul> <p>In contrast to the original ClipCap implementation, which first extracted visual features with CLIP before proceeding to train different architectures, we adopted a simpler approach. Rather than dividing the procedure into two separate steps, we integrated CLIP into the training loop, allowing it to extract features at each training step. This method offers a better overview of the actual training time that such an implementation would take.</p> <p>Our methodology involved running each model through 10 training epochs. To capture the model’s optimal performance, we stored checkpoints throughout and identified the best one based on the lowest validation loss. This optimal checkpoint served as the basis for subsequent model evaluations, ensuring an accurate representation of the model’s capabilities at its peak performance.</p> <h3 id="datasets">Datasets</h3> <p>Choosing good datasets is a critical step for training and evaluating. The notion of “good dataset” in the context of visual-language tasks relies mainly on the diversity of context, topics and entities that the image and captions are covering. Following the original paper, we used the two datasets, COCO and NOCAPS, both considered state-of-the-art datasets for image captioning modelling (.</p> <p>Similar to ClipCap work, we use COCO dataset to train models, and both, COCO and nocaps, to evaluate them. The authors of the ClipCap paper also train and evaluate their models on the large <a href="https://aclanthology.org/P18-1238/" rel="external nofollow noopener" target="_blank">Conceptual Caption (CoCa)</a>, separately from the model trained on COCO. However, due to the substantial computational resources and time required to process CoCa’s extensive collection of over 3 million images, we opted not to use this dataset.</p> <h4 id="coco">COCO</h4> <p><a href="https://arxiv.org/abs/1405.0312" rel="external nofollow noopener" target="_blank">COCO (Common Objects in Context)</a> is a large-scale dataset for image recognition, segmentation, and captioning. It contains over 200K images and 1.2M captions. We used the <a href="https://cs.stanford.edu/people/karpathy/deepimagesent/" rel="external nofollow noopener" target="_blank">Karpathy split</a> for our experiments, which is the same as used in the ClipCap work. The Karpathy split divides the dataset into 113K training images, 5K validation images, and 5K test images.</p> <p>We train our models on the training set, and perform evaluation on the test one.</p> <h4 id="nocaps">NOCAPS</h4> <p><a href="https://arxiv.org/abs/1812.08658" rel="external nofollow noopener" target="_blank">NOCAPS (Novel Object CAPtioning dataset)</a> contains over 166K images and 1.5M captions. It is designed to measure the robustness and generalization of image captioning models to novel objects and concepts. It consists of three subsets: in-domain (containing only COCO classes), near-domain (contains both COCO and novel classes), and out-of-domain (contains only novel classes).</p> <p>Similar to the approach taken in the original ClipCap study, we use a validation set containing 9K images for model evaluation. We paid particular attention to analyzing results from the out-of-domain subset, given its complexity and the challenging tasks it represents. Models that are exclusively trained on COCO data are prone to making significant errors on this subset, thus providing a realistic representation of the performance of COCO-trained models in real-world situations.</p> <h3 id="evaluation">Evaluation</h3> <p>For the caption generation, the exact procedure from the original ClipCap paper is not clearly defined. To ensure consistency across our evaluations, we decided to implement a uniform approach by adopting a greedy search algorithm for all models. This strategy picks the most likely word at each step in the sequence, with the maximum length of the caption set to 67 tokens. We evaluated our models on both quantitative and qualitative metrics.</p> <h4 id="quantitative-evaluation">Quantitative Evaluation</h4> <p>Image captioning is a notoriously difficult task to evaluate due to its inherent ambiguity (Cui et al., 2018). Human evaluation scores are reliable but expensive to obtain and not reproducible. Thus, current image captioning models are usually evaluated with automatic evaluation metrics. Similar to the Clipcap paper, we validate our model over the COCO and nocaps datasets using the <a href="https://arxiv.org/abs/1411.5726" rel="external nofollow noopener" target="_blank">CIDEr</a> and <a href="https://arxiv.org/abs/1607.08822" rel="external nofollow noopener" target="_blank">SPICE</a> metrics. We decided to discard <a href="https://aclanthology.org/P02-1040/" rel="external nofollow noopener" target="_blank">BLEU</a>, <a href="https://aclanthology.org/W04-1013.pdf" rel="external nofollow noopener" target="_blank">ROUGE-L</a> and <a href="https://aclanthology.org/W14-3348/" rel="external nofollow noopener" target="_blank">METEOR</a> now considered out-dated <a href="http://arxiv.org/abs/1806.06422" rel="external nofollow noopener" target="_blank">(Cui et al., 2018)</a>.</p> <p>Most of the metrics in common use for caption evaluation are based on n-gram matching and measure the word overlap and semantic similarity between the generated captions and the reference captions from the datasets. The most known ones are BLEU, ROUGE and METEOR. However, they have been outdated in their evaluation range capabilities, and more complex and robustness-measuring metrics have been developed, which are now considered <a href="http://arxiv.org/abs/1607.08822" rel="external nofollow noopener" target="_blank">state-of-the-art metrics</a>. First, previous metrics were primarily sensitive to n-gram overlap which made them sensitive to the size of the dataset. On the other hand, the novel metrics are size-independent and have been shown to have the <a href="https://doi.org/10.1109/ICCV.2017.100" rel="external nofollow noopener" target="_blank">strongest correlation with human judgments</a>. In particular, To overcome the limitations of existing n-gram based automatic evaluation metrics, SPICE hypothesises that semantic propositional content is an important component of human caption evaluation and estimates caption quality by transforming both candidate and reference captions into a graph-based semantic representation called a scene graph, which make it more content-equivariant. The scene graph explicitly encodes the objects, attributes and relationships found in image captions, abstracting away most of the lexical and syntactic idiosyncrasies of natural language in the process.</p> <p><a href="http://arxiv.org/abs/1411.5726" rel="external nofollow noopener" target="_blank">CIDEr</a> applies term frequency-inverse document frequency (tfidf) weights to n-grams in the candidate and reference sentences, which are then compared by summing their cosine similarity across n-grams. It is worth noting that CIDEr score is the only one that ranges from 0 to infinity. The score is calculated using the average cosine similarity between the candidate sentence and the reference sentences. The score can be greater than 1 if the candidate sentence is more similar to the reference sentences than the reference sentences are to each other. Being an F-score, <a href="http://arxiv.org/abs/1607.08822" rel="external nofollow noopener" target="_blank">SPICE</a> is simple to understand, and easily interpretable as it is naturally bounded between 0 and 1. Unlike CIDEr, SPICE does not use cross-dataset statistics, such as corpus word frequencies, and is therefore equally applicable to both small and large datasets. In summary, while CIDEr focuses on consensus and overall relevance, SPICE centers on semantic propositions. The CIDEr metric assesses how well the machine-generated caption aligns with the consensus annotations of human captions for the same image. If the caption reflects the overall content and significance of the image, and is similar to the consensus captions, it receives a high CIDEr score. The SPICE metric, on the other hand, evaluates the precision and recall of semantic propositions in the machine-generated caption. It analyses how accurately the caption represents the semantic relationships within the image. If the caption correctly identifies the presence of people, a picnic, and a park, and expresses their relationships accurately, it will receive a high SPICE score.</p> <p>We evaluate on COCO locally following the <a href="http://arxiv.org/abs/2004.06165" rel="external nofollow noopener" target="_blank">OSCAR methodology</a>, same as done in ClipCap, while for the NOCAPS dataset, we submit generated captions to the official nocaps challange on the <a href="https://eval.ai/web/challenges/challenge-page/355" rel="external nofollow noopener" target="_blank">EvalAI</a> evaluation server.</p> <p>Additionally, we report <em>total number of parameters</em> of the model, <em>number of trainable parameters</em>, and <em>estimated training time</em>. Less trainable parameters can be linked to faster convergence time, while total number of parameters would influence the inference speed.</p> <h4 id="qualitative-evaluation">Qualitative Evaluation</h4> <p>We conduct the qualitative evaluation by generating the captions of the five first images of the COCO dataset and 3 images of the NOCAPS, one in domain, one near domain and one out of domain. We conduct human evaluation using <a href="https://arxiv.org/pdf/2111.08940.pdf" rel="external nofollow noopener" target="_blank">THumB</a>, a rubric-based protocol that assesses the quality of captions along two main dimensions: precision (how accurate and relevant the caption is) and recall (how much salient information the caption covers) and is designed to promote the human evaluation transparency for qualitative evaluation. First we define the <em>precision</em> of the caption, counting the number of false positives (hallucinations), scored from 0 to 5. Then we define <em>recall</em> which measure how much of the salient information from the image is covered by the caption, also scored from 0 to 5. For instance, an otter is a small animal, and thus small animal is precise. However, it is much less informative (and less natural) than saying an otter. Finally, we add a penalty based on the <a href="https://arxiv.org/pdf/2111.08940.pdf" rel="external nofollow noopener" target="_blank"><em>fluency</em></a> of the sentence from -1 to 0 if there is weird repetitions, misspellings or grammatical errors. There is also the Conciseness and the Inclusive Language to take into account but we did not target these problems.</p> <h2 id="results"><strong>Results</strong></h2> <h4 id="quantitative-evaluation-1">Quantitative evaluation</h4> <p>In the following tables we report CIDEr and SPICE scores on the COCO dataset. Scores for the nocaps dataset are reported for the selected set of models, and can be found at the end of this section.</p> <p>Certain results in the study follow a specific naming convention with the following order: <em>LM, size, visual representation (Pooled, Unpooled), mapper (MLP, Transformer) and finetuning (FT)</em>.</p> <h3 id="baseline-runs">Baseline Runs</h3> <table> <thead> <tr> <th>Language Model</th> <th>LM Size</th> <th>LM Finetuning</th> <th>Mapper</th> <th>CIDEr ↑</th> <th>SPICE ↑</th> <th>Runtime(Hours) ↓</th> <th>Total Parameters(M) ↓</th> <th>Trainable Parameters(M) ↓</th> </tr> </thead> <tbody> <tr> <td>GPT2</td> <td>base</td> <td>Finetuned</td> <td>MLP</td> <td><strong>101.58</strong></td> <td><strong>14.16</strong></td> <td>12.91</td> <td><strong>244</strong></td> <td>156</td> </tr> <tr> <td>GPT2</td> <td>base</td> <td>Frozen</td> <td>Transformer</td> <td>91.57</td> <td>13.45</td> <td><strong>10.77</strong></td> <td>254</td> <td><strong>42</strong></td> </tr> </tbody> </table> <p>Using CIDEr and SPICE scores on COCO dataset as our primary evaluation metrics, we observed results that didn’t precisely match those reported in the original ClipCap paper. It’s important to note here that the disparity might be due to the different methods of caption generation employed, given that the exact procedure was not explicitly stated in the original paper, as previously mentioned.</p> <p>Nonetheless, a significant validation of our approach was that our training and validation loss matched those from the original ClipCap repository when using default parameters. This consistency suggests that our training procedure was robust, despite the discrepancies in caption generation outcomes.</p> <p>As for the training time, there was a noticeable increase in our case compared to the original paper. This increase can be attributed to our decision to include the CLIP model in the forward pass. Unlike the original work, where visual feature extraction was a separate step, we integrated this process within the training loop, as mentioned in an earlier section.</p> <h3 id="comparison-of-language-models">Comparison of Language Models</h3> <table> <thead> <tr> <th>Language Model</th> <th>LM Size</th> <th>LM Finetuning</th> <th>Mapper</th> <th>CIDEr ↑</th> <th>SPICE ↑</th> <th>Runtime(Hours) ↓</th> <th>Total Parameters(M) ↓</th> <th>Trainable Parameters(M) ↓</th> </tr> </thead> <tbody> <tr> <td>GPT2</td> <td>base</td> <td>Finetuned</td> <td>MLP</td> <td>101.58</td> <td>14.16</td> <td>12.91</td> <td>244</td> <td>156</td> </tr> <tr> <td>FLAN-T5</td> <td>base</td> <td>Finetuned</td> <td>MLP</td> <td>105.52</td> <td>19.49</td> <td>13.16</td> <td>367</td> <td>141</td> </tr> <tr> <td>FLAN-T5 (Decoder Only)</td> <td>base</td> <td>Finetuned</td> <td>MLP</td> <td><strong>106.8</strong></td> <td><strong>19.96</strong></td> <td>12.7</td> <td>282</td> <td>194</td> </tr> <tr> <td>FLAN-T5</td> <td>small</td> <td>Finetuned</td> <td>MLP</td> <td>95.13</td> <td>18.08</td> <td>6.6</td> <td>186</td> <td>57</td> </tr> <tr> <td>FLAN-T5 (Decoder Only)</td> <td>small</td> <td>Finetuned</td> <td>MLP</td> <td>104.44</td> <td>19.85</td> <td>6.8</td> <td>168</td> <td>80</td> </tr> <tr> <td>GPT2</td> <td>base</td> <td>Frozen</td> <td>Transformer</td> <td>91.57</td> <td>13.45</td> <td>10.77</td> <td>254</td> <td>42</td> </tr> <tr> <td>FLAN-T5 (Decoder Only)</td> <td>base</td> <td>Frozen</td> <td>Transformer</td> <td>93.62</td> <td>18.87</td> <td>10.4</td> <td>292</td> <td>42</td> </tr> <tr> <td>FLAN-T5</td> <td>base</td> <td>Frozen</td> <td>Transformer</td> <td>91.56</td> <td>17.97</td> <td>11.7</td> <td>377</td> <td>42</td> </tr> <tr> <td>FLAN-T5</td> <td>small</td> <td>Frozen</td> <td>Transformer</td> <td>90.33</td> <td>17.41</td> <td>7.1</td> <td>184</td> <td><strong>19</strong></td> </tr> <tr> <td>FLAN-T5 (Decoder Only)</td> <td>small</td> <td>Frozen</td> <td>Transformer</td> <td>93.19</td> <td>18.2</td> <td><strong>6.44</strong></td> <td><strong>165</strong></td> <td><strong>19</strong></td> </tr> </tbody> </table> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/MLP_transformer_1.png" alt="MLP_transformer_1"></p> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/MLP_transformer_2.png" alt="MLP_transformer_2"></p> <p>When comparing the results for different sizes of the FLAN-T5 decoder with the Transformer Mapper, we observe minimal changes. Furthermore, the SPICE scores consistently favor the FLAN-T5-based models, particularly the Decoder only variants.</p> <p>In terms of the CIDEr score, all FLAN-T5 variations outperform the baseline model, except for the small sized model. Notably, FLAN-T5 Decoder only models achieve higher scores than the full FLAN-T5 counterpart. Among the Decoder only models, the base size demonstrates the best performance on both metrics. Additionally, even the small version of the finetuned FLAN-T5 surpasses the performance of the best baseline model, while reducing the trainable parameters by almost half.</p> <h3 id="analyzing-the-utility-of-unpooled-visual-representations">Analyzing the Utility of Unpooled Visual Representations</h3> <table> <thead> <tr> <th>Language Model</th> <th>LM Size</th> <th>LM Finetuning</th> <th>Image Embeddings</th> <th>CIDEr ↑</th> <th>SPICE ↑</th> <th>Runtime(Hours) ↓</th> <th>Total Parameters(M) ↓</th> <th>Trainable Parameters(M) ↓</th> </tr> </thead> <tbody> <tr> <td>GPT2</td> <td>base</td> <td>Frozen</td> <td>Pooled</td> <td>92.06</td> <td>13.31</td> <td>9.9</td> <td>244</td> <td>31</td> </tr> <tr> <td>GPT2</td> <td>base</td> <td>Finetuned</td> <td>Pooled</td> <td>101.59</td> <td>14.16</td> <td>12.9</td> <td>244</td> <td>156</td> </tr> <tr> <td>FLAN-T5</td> <td>small</td> <td>Finetuned</td> <td>Pooled</td> <td>95.14</td> <td>18.08</td> <td>6.6</td> <td>186</td> <td>57</td> </tr> <tr> <td>FLAN-T5</td> <td>base</td> <td>Finetuned</td> <td>Pooled</td> <td>105.52</td> <td>19.49</td> <td>13.2</td> <td>367</td> <td>141</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>small</td> <td>Finetuned</td> <td>Pooled</td> <td>104.44</td> <td>19.85</td> <td>6.8</td> <td>168</td> <td>80</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>base</td> <td>Finetuned</td> <td>Pooled</td> <td>106.80</td> <td>19.96</td> <td>12.7</td> <td>282</td> <td>194</td> </tr> <tr> <td>GPT2</td> <td>base</td> <td>Frozen</td> <td>Unpooled</td> <td>84.52</td> <td>12.69</td> <td>15.8</td> <td>218</td> <td>6</td> </tr> <tr> <td>GPT2</td> <td>base</td> <td>Finetuned</td> <td>Unpooled</td> <td>105.88</td> <td>14.69</td> <td>19.8</td> <td>218</td> <td>130</td> </tr> <tr> <td>FLAN-T5</td> <td>small</td> <td>Finetuned</td> <td>Unpooled</td> <td>93.81</td> <td>18.08</td> <td>8.2</td> <td>170</td> <td>40</td> </tr> <tr> <td>FLAN-T5</td> <td>base</td> <td>Finetuned</td> <td>Unpooled</td> <td>107.65</td> <td>19.94</td> <td>18.9</td> <td>341</td> <td>116</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>small</td> <td>Frozen</td> <td>Unpooled</td> <td>91.23</td> <td>18.03</td> <td><strong>6</strong></td> <td><strong>151</strong></td> <td><strong>5</strong></td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>small</td> <td>Finetuned</td> <td>Unpooled</td> <td>103.59</td> <td>19.64</td> <td>7.6</td> <td><strong>151</strong></td> <td>63</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>base</td> <td>Frozen</td> <td>Unpooled</td> <td>95.78</td> <td>18.77</td> <td>10.7</td> <td>256</td> <td>6</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>base</td> <td>Finetuned</td> <td>Unpooled</td> <td><strong>108.81</strong></td> <td><strong>20.24</strong></td> <td>14.2</td> <td>256</td> <td>169</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>large</td> <td>Frozen</td> <td>Unpooled</td> <td>99.31</td> <td>19.21</td> <td>22.8</td> <td>570</td> <td>7</td> </tr> </tbody> </table> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/MLP_transformer_2.png" alt="image pooled unpooled"></p> <p>We observe a trend where the use of unpooled representations enhances the performance of models with finetuned LMs, while it has a negative impact on frozen LM architectures.</p> <p>Additionally, we notice that employing larger LMs can improve the performance of frozen FLAN-T5 Decoder models while maintaining a similar number of trainable parameters. <strong>Notably, with only <em>7 million</em> trainable parameters compared to <em>156 million</em>, we achieve comparable CIDEr scores and better SPICE scores than the finetuned GPT-2 based baseline.</strong></p> <h4 id="ablation-study-on-hidden-layer-size">Ablation Study on Hidden Layer Size</h4> <p>Here we perform an ablation study investigating impact of the hidden layer size of the MLP on the performance.</p> <table> <thead> <tr> <th>Language Model</th> <th>MLP Hidden Layer Size</th> <th>CIDEr ↑</th> <th>SPICE ↑</th> <th>Runtime(Hours) ↓</th> <th>Total Parameters(M) ↓</th> <th>Trainable Parameters(M) ↓</th> </tr> </thead> <tbody> <tr> <td>FLAN-T5 (Decoder)</td> <td>32</td> <td>74.38</td> <td>15.17</td> <td><strong>5.4</strong></td> <td><strong>146</strong></td> <td><strong>0.042</strong></td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>128</td> <td>86.33</td> <td>17.14</td> <td><strong>5.4</strong></td> <td><strong>146</strong></td> <td>0.164</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>256</td> <td>86.49</td> <td>17.24</td> <td><strong>5.5</strong></td> <td><strong>146</strong></td> <td>0.328</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>512</td> <td>90.82</td> <td>17.89</td> <td><strong>5.4</strong></td> <td>147</td> <td>0.656</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>2048</td> <td>91.22</td> <td><strong>18.08</strong></td> <td>5.6</td> <td>149</td> <td>2.624</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>3840</td> <td><strong>91.23</strong></td> <td>18.03</td> <td>6</td> <td>151</td> <td>4.92</td> </tr> </tbody> </table> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/scores_hidden_layers.png" alt=""></p> <p>We observe that performance exhibits a sharp increase for hidden layer sizes below 512 indicating their impact on the model’s performance. However, once this threshold is surpassed, further increases in hidden layer size result in similar performance levels despite the addition of a substantial number of trainable parameters. From these findings, we can infer that a hidden layer size of 512 would be optimal for this specific use case.</p> <h3 id="application-of-lora">Application of LoRA</h3> <p>We apply LoRA to the baseline architecture (MLP mapper + GPT2) and our best-performing model, FLAN-T5 LM with a decoder only architecture and a MLP mapper processing unpooled CLIP embeddings. We test this for 3 FLAN-T5 sizes - base, small, and large. Additionally, we attempt to analyse how the application of LoRA to different layers of the language model could affect the results, for which we select 2 cases - applying LoRA to all Linear layers of the LM, and applying it a smaller subset of layers. In case of GPT2, we apply it to “c_attn” and “c_proj”, and in case of FLAN-T5, to the “q” and “v” matrices. Motivated by the results from <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">Hu et al. (2021)</a>, we decided to apply LoRA to this specific subset of layers in the LM.</p> <p>For the LoRA experiments, we use the following hyperparameters</p> <table> <thead> <tr> <th>Hyperparameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Rank</td> <td>4</td> </tr> <tr> <td>Alpha</td> <td>32</td> </tr> <tr> <td>Dropout</td> <td>0.01</td> </tr> </tbody> </table> <table> <thead> <tr> <th>Language Model</th> <th>Size</th> <th>Finetuning</th> <th>LM Total Parameters(M) ↓</th> <th>LM Trainable Parameters(M) ↓</th> <th>Reduction Trainable Parameters(%) ↑</th> <th>CIDEr ↑</th> <th>SPICE ↑</th> <th>Runtime(Hours) ↓</th> <th>Total Parameters(M) ↓</th> <th>Trainable Parameters(M) ↓</th> </tr> </thead> <tbody> <tr> <td>GPT2</td> <td>base</td> <td>Full LM</td> <td>125</td> <td>125</td> <td>0</td> <td>101.59</td> <td>14.16</td> <td>12.9</td> <td>243.76</td> <td>155.91</td> </tr> <tr> <td>GPT2</td> <td>base</td> <td>LORA (All Layers)</td> <td>125</td> <td>0.794</td> <td>99.36</td> <td>96.06</td> <td>13.58</td> <td>11.5</td> <td>244.552</td> <td>32.263</td> </tr> <tr> <td>GPT2</td> <td>base</td> <td>LORA (Subset of Layers)</td> <td>125</td> <td>0.406</td> <td>99.68</td> <td>95.98</td> <td>13.67</td> <td>11</td> <td>244.163</td> <td>31.874</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>base</td> <td>Full LM</td> <td>163</td> <td>163</td> <td>0</td> <td><strong>108.81</strong></td> <td><strong>20.23</strong></td> <td>14.2</td> <td>256.376</td> <td>168.526</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>base</td> <td>LORA (All Layers)</td> <td>163</td> <td>1.127</td> <td>99.31</td> <td>102.29</td> <td>19.43</td> <td>12.2</td> <td>257.503</td> <td>7.03</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>base</td> <td>LORA (Subset of Layers)</td> <td>163</td> <td>0.295</td> <td>99.82</td> <td>101.12</td> <td>19.199</td> <td>10.3</td> <td>256.671</td> <td>6.198</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>small</td> <td>Full LM</td> <td><strong>58.5</strong></td> <td>58.5</td> <td>0</td> <td>103.60</td> <td>19.64</td> <td>7.6</td> <td><strong>150.847</strong></td> <td>62.997</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>small</td> <td>LORA (All Layers)</td> <td><strong>58.5</strong></td> <td><strong>0.507</strong></td> <td>99.13</td> <td>98.69</td> <td>18.94</td> <td>7.4</td> <td>151.354</td> <td>5.427</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>small</td> <td>LORA (Subset of Layers)</td> <td><strong>58.5</strong></td> <td>0.115</td> <td>99.8</td> <td>95.09</td> <td>18.73</td> <td><strong>6.1</strong></td> <td>150.961</td> <td><strong>5.034</strong></td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>large</td> <td>LORA (All Layers)</td> <td>475</td> <td>2.811</td> <td>99.41</td> <td>103.46</td> <td>19.64</td> <td>28.4</td> <td>572.365</td> <td>9.698</td> </tr> <tr> <td>FLAN-T5 (Decoder)</td> <td>large</td> <td>LORA (Subset of Layers)</td> <td>475</td> <td>0.786</td> <td><strong>99.83</strong></td> <td>102.40</td> <td>19.76</td> <td>23.7</td> <td>570.34</td> <td>7.673</td> </tr> </tbody> </table> <table> <thead> <tr> <th> </th> <th>Full LM</th> <th> </th> <th> </th> <th>LORA (All Layers)</th> <th> </th> <th> </th> <th>LORA (Subset of Layers)</th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td> </td> <td>LM Trainable Parameters(M)</td> <td>CIDEr</td> <td>SPICE</td> <td>LM Trainable Parameters(M)</td> <td>CIDEr</td> <td>SPICE</td> <td>LM Trainable Parameters(M)</td> <td>CIDEr</td> <td>SPICE</td> </tr> <tr> <td>GPT2_base</td> <td>125</td> <td>101.59</td> <td>14.16</td> <td>0.794 <span style="color:green"> <strong>(0.64%)</strong> </span> </td> <td>96.06 <span style="color:red"> <strong>(-5.53)</strong> </span> </td> <td>13.58 <span style="color:red"> <strong>(-0.58)</strong> </span> </td> <td>0.406 <span style="color:green"> <strong>(0.32%)</strong> </span> </td> <td>95.98 <span style="color:red"> <strong>(-5.61)</strong> </span> </td> <td>13.67 <span style="color:red"> <strong>(-0.49)</strong> </span> </td> </tr> <tr> <td>FLAN-T5 (Decoder)-base</td> <td>163</td> <td>108.81</td> <td>20.24</td> <td>1.127 <span style="color:green"> <strong>(0.69%)</strong> </span> </td> <td>102.29 <span style="color:red"> <strong>(-6.52)</strong> </span> </td> <td>19.43 <span style="color:red"> <strong>(-0.81)</strong> </span> </td> <td>0.295 <span style="color:green"> <strong>(0.18%)</strong> </span> </td> <td>101.12 <span style="color:red"> <strong>(-7.68)</strong> </span> </td> <td>19.2 <span style="color:red"> <strong>(-1.04)</strong> </span> </td> </tr> <tr> <td>FLAN-T5 (Decoder)-small</td> <td>58.5</td> <td>103.60</td> <td>19.64</td> <td>0.507 <span style="color:green"> <strong>(0.87%)</strong> </span> </td> <td>98.69 <span style="color:red"> <strong>(-4.91)</strong> </span> </td> <td>18.94 <span style="color:red"> <strong>(-0.7)</strong> </span> </td> <td>0.115 <span style="color:green"> <strong>(0.2%)</strong> </span> </td> <td>95.09 <span style="color:red"> <strong>(-8.5)</strong> </span> </td> <td>18.73 <span style="color:red"> <strong>(-0.9)</strong> </span> </td> </tr> </tbody> </table> <p>We can observe from the obtained results that the models when trained with LoRA, shows a significant deacrease in trainable parameters (~99.5% reduction on an average), while achieving a comparable but lower scores on both CIDEr and SPICE metrics.</p> <h3 id="t5-weights">T5 Weights</h3> <p>We conducted a performance comparison on selected FLAN-T5 architectures with different weights: FLAN-T5 and original T5. To assess this comparison, we have selected the best performing model on the COCO dataset, which is the finetuned FLAN-T5 Decoder only with unpooled representations, and its version with the frozen LM.</p> <p><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/Flan_flant5.png" alt=""></p> <p>It’s evident that FLAN-T5 yields better results than T5 version for finetuned and frozen LM, with substantial change when LM is frozen. The analysis of the generated captions can provide explanations on these results.</p> <table> <thead> <tr> <th>Images</th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_1.png" alt=""></th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_2.png" alt=""></th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_3.png" alt=""></th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_4.png" alt=""></th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_5.png" alt=""></th> </tr> </thead> <tbody> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, No FT</td> <td>A man on a bike with a backpack. <br> </td> <td>A girl is eating a piece of cake. <br> </td> <td>A man standing next to a train on the tracks. <br> </td> <td>A kitchen with a sink, a window and a window. <br> </td> <td>A group of stacked wooden spoons sitting on a table. <br> </td> </tr> <tr> <td>T5 (Decoder Only), base, Unpooled, MLP, No FT</td> <td>A man on a bike on a bike. <br> </td> <td>A girl girl eating eating a mouth mouth mouth mouth mouth mouth mouth mouth mouth mouth mouth <br> </td> <td>A person is standing on a train. <br> </td> <td>A kitchen with a kitchen with a kitchen and a kitchen. <br> </td> <td>A few few few few few few few few few few few few few few few few few few few few <br> </td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, FT</td> <td>A man riding a motorcycle down a dirt road. <br> </td> <td>A girl is eating a piece of cake with a candle. <br> </td> <td>A man standing next to a train on a track. <br> </td> <td>A kitchen with a stove, sink, and window. <br> </td> <td>A group of wooden spoons sitting on a wooden table. <br> </td> </tr> <tr> <td>T5 (Decoder Only), base, Unpooled, MLP, FT</td> <td>A man riding a dirt bike down a dirt road. <br> </td> <td>A woman is eating a piece of cake. <br> </td> <td>A man standing next to a train on a track. <br> </td> <td>A kitchen with a sink, microwave, and window. <br> </td> <td>A group of wooden wooden utensils sitting on a wooden table. <br> </td> </tr> </tbody> </table> <p>We can see that the model utilizing the frozen T5 produces repetitive and incoherent captions. In the comparison of models with finetuned LM, both models yield captions of similar quality, with the T5 version having only a single repetition for the last picture. However, models with the FLAN-T5 weights outperform its T5 counterpart in both cases.</p> <h3 id="nocaps-results">Nocaps results</h3> <table> <thead> <tr> <th>Models</th> <th>CIDEr_entire</th> <th>SPICE_entire</th> <th>CIDEr_in-domain</th> <th>SPICE_in-domain</th> <th>CIDEr_near-domain</th> <th>SPICE_near-domain</th> <th>CIDEr_out-domain</th> <th>SPICE_out-domain</th> </tr> </thead> <tbody> <tr> <td>GPT-2, base, Unpooled, MLP, FT</td> <td><strong>73.86</strong></td> <td><strong>11.78</strong></td> <td><strong>88.98</strong></td> <td><strong>12.71</strong></td> <td><strong>76.51</strong></td> <td><strong>12.04</strong></td> <td><strong>54.59</strong></td> <td><strong>10.09</strong></td> </tr> <tr> <td>GPT-2, base, Pooled, Transformer, No FT</td> <td>60.48</td> <td>10.3</td> <td>77.18</td> <td>11.4</td> <td>61.57</td> <td>10.38</td> <td>45.14</td> <td>9.03</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, No FT, with LoRA on all layers</td> <td>62.6</td> <td>10.66</td> <td>77.76</td> <td>11.58</td> <td>64.24</td> <td>10.84</td> <td>46.56</td> <td>9.23</td> </tr> <tr> <td>GPT-2, base, Pooled, MLP, No FT</td> <td>61.81</td> <td>10.23</td> <td>78.08</td> <td>11.58</td> <td>63.11</td> <td>10.35</td> <td>46.13</td> <td>8.65</td> </tr> <tr> <td>GPT-2, base, Pooled, MLP, FT</td> <td>66.78</td> <td>10.88</td> <td>82.27</td> <td>12</td> <td>68.53</td> <td>11.05</td> <td>50.2</td> <td>9.33</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, No FT</td> <td>55.91</td> <td>10.28</td> <td>71.79</td> <td>11.44</td> <td>58.3</td> <td>10.45</td> <td>37</td> <td>8.58</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, FT</td> <td>67.36</td> <td>11.3</td> <td>83.32</td> <td>12.41</td> <td>69.05</td> <td>11.43</td> <td>50.63</td> <td>9.92</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), large, Unpooled, MLP, No FT</td> <td>61.55</td> <td>10.62</td> <td>77.31</td> <td>11.36</td> <td>63.01</td> <td>10.85</td> <td>45.67</td> <td>9.13</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), small, Unpooled, MLP, No FT</td> <td>50.65</td> <td>9.76</td> <td>68.17</td> <td>11</td> <td>53.08</td> <td>9.9</td> <td>30.39</td> <td>8.01</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), small, Unpooled, MLP, FT</td> <td>60.48</td> <td>10.66</td> <td>79.25</td> <td>11.79</td> <td>62.85</td> <td>10.9</td> <td>39.58</td> <td>8.8</td> </tr> <tr> <td>FLAN-T5, base, Pooled, MLP, FT</td> <td>59.81</td> <td>10.23</td> <td>78.22</td> <td>11.42</td> <td>61.49</td> <td>10.39</td> <td>41.35</td> <td>8.62</td> </tr> </tbody> </table> <p>Surprisingly, the highest performing model on nocaps dataset is finetuned GPT-2 based architecture with unpooled representations, which outperforms all reported model in the original ClipCap paper. Again, fine-tuned models generally perform better than their non-fine-tuned equivalent. On all its different implementations the FLAN-T5 Decoder only models consistently perform well, with scores ranging from 55.91 to 67.36 for nocaps CIDEr and from 9.76 to 11.3 for nocaps SPICE. Larger model sizes generally lead to better performance compared to smaller models.</p> <h3 id="qualitative-results">Qualitative Results</h3> <p>Using THumb to do the human evaluation on COCO and NOCAPS we define precision (P : from 0 to 5), which refers to how accurate and relevant the caption is, and recall (R : from 0 to 5), which measures the extent to which the caption covers important information from the image. Additionally, we considered the fluency (F : between -1 and 0) of the sentence. The total score (Total) is the average between precision and recall adjusted by the fluency penalty. Each caption is followed by a table of the different scores as following : [Total, P, R, F]</p> <h5 id="coco-1">COCO</h5> <table> <thead> <tr> <th>Images</th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_1.png" alt="coco_1"></th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_2.png" alt="coco_2"></th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_3.png" alt="coco_3"></th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_4.png" alt="coco_4"></th> <th><img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/coco_5.png" alt="coco_5"></th> </tr> </thead> <tbody> <tr> <td>GPT-2, base, Pooled, MLP, No FT [<em>Clipcap MLP</em>]</td> <td>A man riding a motorcycle on a dirt road.<br> [<strong>4</strong>, 5, 3, 0]</td> <td>A woman is holding a cake with a child in it. <br>[3, 3, 3, 0]</td> <td>A man is standing on a train with a red train. <br>[3.5, 3, 4, 0]</td> <td>A kitchen with a stove and a sink. <br>[4.5, 5, 4, 0]</td> <td>A wooden table with many wooden pieces. <br>[4, 5, 3, 0]</td> </tr> <tr> <td>GPT-2, base, Pooled, MLP, FT</td> <td>A man riding a motorcycle down a dirt road. <br>[<strong>4</strong>, 5, 3, 0]</td> <td>A woman is eating a chocolate cake with a candle. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A man walking past a train on a train track. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A kitchen with a stove, sink, and window. <br>[<strong>5</strong>, 5, 5, 0]</td> <td>A bunch of wooden bowls and spoons on a table. <br>[4.5, 5, 4, 0]</td> </tr> <tr> <td>GPT-2, base, Unpooled, MLP, FT</td> <td>A man riding a motorcycle down a dirt road. <br>[4, 5, 3, 0]</td> <td>A woman eating food from a bowl with a candle. <br>[<strong>4</strong>, 5, 3, 0]</td> <td>A man standing next to a train on a train track. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A kitchen with a sink, stove, and window. <br>[<strong>5</strong>, 5, 5, 0]</td> <td>A bunch of wooden stools lined up. <br>[4.5, 5, 4, 0]</td> </tr> <tr> <td>GPT-2, base, Pooled, Transformer, No FT [<em>Clipcap Transformer</em>]</td> <td>A man riding a motorcycle on a dirt road. <br>[<strong>4</strong>, 5, 3, 0]</td> <td>A woman is eating a cake with a cake on it. <br>[3, 3, 3, 0]</td> <td>A man is walking down a train track. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A kitchen with a stove, stove top, and a sink. <br>[4.5, 5, 5, 0]</td> <td>A row of wooden tools sitting on a table. <br>[4.5, 5, 4, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, No FT</td> <td>A man on a bike with a backpack. <br>[3.5, 4, 3, 0]</td> <td>A girl is eating a piece of cake. <br>[3.5, 4, 3, 0]</td> <td>A man standing next to a train on the tracks. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A kitchen with a sink, a window and a window. <br>[4, 5, 4, 0]</td> <td>A group of stacked wooden spoons sitting on a table. <br>[<strong>5</strong>, 5, 5, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, FT</td> <td>A man riding a motorcycle down a dirt road. <br>[<strong>4</strong>, 5, 3, 0]</td> <td>A girl is eating a piece of cake with a candle. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A man standing next to a train on a track. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A kitchen with a stove, sink, and window. <br>[<strong>5</strong>, 5, 5, 0]</td> <td>A group of wooden spoons sitting on a wooden table. <br>[4.5, 5, 4, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, No FT, with LoRA on all layers</td> <td>A man is riding a bicycle on a dirt path. <br>[3.5, 4, 3, 0]</td> <td>A woman is eating a cake with a fork. <br>[3.5, 4, 3, 0]</td> <td>A man standing next to a train on a track. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A kitchen with a stove, sink, and window. <br>[<strong>5</strong>, 5, 5, 0]</td> <td>A group of wooden utensils are lined up. <br>[4.5, 5, 4, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), large, Unpooled, MLP, No FT</td> <td>A man riding a motorcycle on a dirt road. <br>[<strong>4</strong>, 5, 3, 0]</td> <td>A woman eating a cake with a candle in it. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A man standing next to a train on a train track. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A kitchen with a stove and a sink. <br>[4.5, 5, 4, 0]</td> <td>A bunch of wooden spoons are stacked on top of each other. <br>[3.5, 3, 4, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), small, Unpooled, MLP, No FT</td> <td>A man riding a motorcycle on a dirt road. <br>[<strong>4</strong>, 5, 3, 0]</td> <td>A girl is eating a cake with a bowl of food. <br>[3, 3, 3, 0]</td> <td>A man is standing next to a train on a train. <br>[3, 3, 4, 0]</td> <td>A kitchen with a sink, sink, and a window. <br>[4, 5, 4, 0]</td> <td>A bunch of different types of skateboards are sitting on a table. <br>[3, 2, 4, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), small, Unpooled, MLP, FT</td> <td>A man riding a motorcycle on a dirt road. <br>[<strong>4</strong>, 5, 3, 0]</td> <td>A woman is eating a cake with a knife. <br>[2.5, 2, 3, 0]</td> <td>A man standing next to a train on a track. <br>[<strong>4</strong>, 4, 4, 0]</td> <td>A kitchen with a sink, stove, and a window. <br>[<strong>5</strong>, 5, 5, 0]</td> <td>A group of wooden stools with a variety of knives. <br>[3.5, 3, 4, 0]</td> </tr> <tr> <td>FLAN-T5, base, Pooled, MLP, FT</td> <td>A man riding a motorcycle down a dirt road. <br>[<strong>4</strong>, 5, 3, 0]</td> <td>A woman is cutting a cake with a fork. <br>[2.5, 2, 3, 0]</td> <td>A train is stopped at a train station. <br>[3, 3, 3, 0]</td> <td>A kitchen with a stove, oven, and sink. <br>[<strong>5</strong>, 5, 5, 0]</td> <td>A bunch of wooden spoons are sitting on a table. <br>[4.5, 5, 4, 0]</td> </tr> </tbody> </table> <p>The FLAN-T5 (Decoder Only) model variations, utilizing unpooled representations and an MLP mapper consistently achieve the highest scores on the COCO dataset, exhibiting slightly more precise captions. These models, along with the rest of the models, yield excellent results on the COCO dataset. The observed performance aligns well with the CIDEr and SPICE scores.</p> <h5 id="nocaps-1">Nocaps</h5> <table> <thead> <tr> <th>Images</th> <th> <em>In domain</em> <img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/nocaps_1.jpg" alt="nocaps_1"> </th> <th> <em>Near-domain</em> <img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/nocaps_2.jpg" alt="nocaps_2"> </th> <th> <em>Out-domain</em> <img src="https://github.com/dkopi/clipcap-evolved/blob/main/images/nocaps_3.jpg" alt="nocaps_3"> </th> </tr> </thead> <tbody> <tr> <td>GPT-2, base, Pooled, MLP, No FT [<em>Clipcap MLP</em>]</td> <td>A boy standing in front of a wooden bench. <br>[4, 4, 4, 0]</td> <td>A man riding on a elephant with a man on top. <br>[1.5, 2, 2, -0.5]</td> <td>A coffee and a bottle of soda on a table. <br>[3.5, 4, 3, 0]</td> </tr> <tr> <td>GPT-2, base, Pooled, MLP, FT</td> <td>A young boy standing next to a parked motorcycle. <br>[3.5, 3, 4,0 ]</td> <td>A man riding on the back of an elephant. <br>[2, 2, 2, 0]</td> <td>A table topped with a cup of coffee and a box of ice cream. <br>[2.5, 2, 3, 0]</td> </tr> <tr> <td>GPT-2, base, Unpooled, MLP, FT</td> <td>A little boy standing on a sidewalk holding a toothbrush. <br>[3, 2, 4, 0]</td> <td>A man riding on the back of an elephant. <br>[2, 2, 2, 0]</td> <td>A table topped with a bag of drinks and a bag of snacks. <br>[3.5, 4, 3, 0]</td> </tr> <tr> <td>GPT-2, base, Pooled, Transformer, No FT [<em>Clipcap Transformer</em>]</td> <td>A young boy is standing in a wooden bench. <br>[3.5, 4, 4, -0.5]</td> <td>A man riding on top of an elephant with a man on top. <br>[1.5, 2, 2, -0.5]</td> <td>A table with a bunch of drinks and a cup of coffee. <br>[3, 3, 3, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, No FT</td> <td>A little boy is standing on a sidewalk. <br>[4, 4, 4, 0]</td> <td>An elephant with a man on it’s back. <br>[<strong>2.5</strong>, 3, 2, 0]</td> <td>A bunch of sodas and a mug of beer. <br>[3, 3, 3, ]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, FT</td> <td>A young boy standing on a sidewalk holding a tennis racket. <br>[3.5, 3, 4, 0]</td> <td>A man riding on the back of an elephant. <br>[2, 2, 2, 0]</td> <td>A table topped with a cup of coffee and a soda. <br>[3, 3, 3, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), base, Unpooled, MLP, No FT, with LoRA on all layers</td> <td>A little boy is standing in the street. <br>[<strong>4.5</strong>, 5, 4, 0]</td> <td>A man riding an elephant on a dirt road. <br>[<strong>2.5</strong>, 3, 2, 0]</td> <td>A variety of different types of drinks are on a table. <br>[<strong>4.5</strong>, 5, 4, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), large, Unpooled, MLP, No FT</td> <td>A young child standing on a sidewalk with a hat. <br>[3.5, 3, 4, 0]</td> <td>A man is riding on top of an elephant. <br>[2, 2, 2, 0]</td> <td>A can of soda and a bottle of a cola. <br>[3, 3, 3, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), small, Unpooled, MLP, No FT</td> <td>A little boy in a shirt and a shirt. <br>[4, 5, 4, -0.5]</td> <td>A large elephant with a tusk on its back. <br>[<strong>2.5</strong>, 3, 2, 0]</td> <td>A group of various types of food and drinks. <br>[4, 5, 3, 0]</td> </tr> <tr> <td>FLAN-T5 (Decoder Only), small, Unpooled, MLP, FT</td> <td>A young boy is standing on the sidewalk. <br>[4, 4, 4, 0]</td> <td>A man riding on the back of an elephant. <br>[2, 2, 2, 0]</td> <td>A bunch of drinks and a bottle of Coca Cola. <br>[3, 3, 3, 0]</td> </tr> <tr> <td>FLAN-T5, base, Pooled, MLP, FT</td> <td>A young boy wearing a tie and a hat. <br>[3, 2, 4, 0]</td> <td>A man riding an elephant on a dirt road. <br>[2, 2, 2, 0]</td> <td>A table with a cup of coffee, a drink and a bottle of water. <br>[2.5, 2, 3, 0]</td> </tr> </tbody> </table> <p>The FLAN-T5 decoder model (base), utilizing unpooled representations, an MLP mapper and applying LoRA across all layers, consistently performed best across the provided domains. Its captions exhibited higher consistency, richness, and level of detail. Moreover, it achieved the highest scores for “in-domain”, “near-domain” and “out-of-domain” images, indicating its strong generalization capabilities beyond the specific training domain while still being very good at trained tasks. This model’s ability to generate accurate descriptions across various domains (not limited to “in-domain” images) highlights its versatility and adaptability.</p> <p>On the other hand, the finetuned FLAN-T5 (base), which utilized pooled representations from an MLP mapper, along with the original Clipcap transformer approach, exhibited the poorest captions. For instance, these models generated descriptions like “A table with a cup of coffee, a drink, and a bottle of water” even when there were no actual cup of coffee or bottle of water present in the image. Furthermore, they produced repetitive captions such as “A man riding on top of an elephant with a man on top.”</p> <h2 id="discussion"><strong>Discussion</strong></h2> <p>In our evaluation, the FLAN-T5 models consistently achieve higher scores on the COCO dataset, indicating their strong performance in generating captions for a wide range of images. However, it is noteworthy that the GPT-2 based model with unpooled CLIP representations and fine-tuning emerges as the best-performing model on the Nocaps dataset.</p> <p>The distinguishing factor between these models lies primarily in the LM, while the other parameters remain consistent. One possible explanation for this observation is that, despite FLAN-T5 being trained on a diverse set of tasks, it may not possess the same level of robustness as GPT2, which benefits from being trained on a vast and diverse range of data. This suggests that the diversity of data that the LM has been exposed to may play a crucial role in the it’s ability to generalize and produce accurate captions across different datasets.</p> <p>Another notable observation is the impact of using unpooled representations projected with an MLP on the model’s performance. Specifically, we observed that it generally improves results when finetuning the language model, but worsens performance otherwise. This can be attributed to the fact that unpooled representations are processed individually by MLP, providing mostly local information about the image. As a result, the language model needs to adapt to effectively utilize this information, whereas the baseline method can generate suitable captions relying on the prefix alone, which contains processed information from pooled representations.</p> <p>Our findings also indicate that employing an MLP with a hidden layer size of just 512 neurons is sufficient for proper projection of CLIP’s unpooled representations into the FLAN-T5 decoder’s representation space. This yields satisfactory results while keeping the number of trainable parameters at only 7 million.</p> <p>Furthermore, our comparison of T5 and FLAN-T5 weights revealed a significant drop in performance for the frozen T5 version. This confirms that FLAN, as a finetuning method, greatly enhances the model’s robustness and its ability to handle new tasks.</p> <p>In conclusion, we achieved better performance on COCO and nocaps datasets with less trainable parameters through our proposed approaches. FLAN-T5 architectures consistently demonstrated superior results, especially Decoder only variants. By leveraging unpooled representations and applying fine-tuning, the performance gain provides empirical evidence supporting our hypothesis that unpooled representations contain useful image-specific information that can be effectively utilized for captioning. We found that using LoRA, we can reduce the number of trainable parameters significantly, while still achieving similar results to the baselines, with a slight drop in performance. We believe, these findings can contribute to advancing the field of image captioning and provide valuable insights for developing more efficient and accurate models in the future.</p> <h2 id="further-work">Further Work</h2> <p>There is considerable potential for further exploration and refinement in our modified ClipCap model. One potential avenue for future research involves experimenting with the multi-layer perceptron (MLP) for unpooled representations. Changing variables such as depth and activation functions could have a significant impact on performance and offer valuable insights into the optimal configuration for this element of the model.</p> <p>In addition, we see value in examining the integration of global information alongside the unpooled CLIP representations. The hypothesis is that in the current approach, the processed visual tokens contain mostly information about local content of an image patch, which could be enhanced by providing a broader context. Integrating global information could potentially deliver a more comprehensive picture of the visual data, thus further improving captioning performance. However, this remains a hypothesis and will require rigorous testing and validation.</p> <h2 id="citation">Citation</h2> <p>Cited as:</p> <blockquote> <p>Varghese, D., Kopiczko, D., Thamma, A., Goswami, P., Pelletreau-Duris, T. (Mar 2023). ClipCap Evolved - Bridging the Gap Between Modalities. https://dhevarghese.github.io/blog/2023/clipcap-evolved/</p> </blockquote> <p>Or</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{varghese2023ccEvolved,
  title   = "ClipCap Evolved.",
  author  = "Varghese, D.",
  journal = "dhevarghese.github.io",
  year    = "2023",
  month   = "Mar",
  url     = "https://dhevarghese.github.io/blog/2023/clipcap-evolved/"
}
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"dhevarghese/dhevarghese.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © 2024 Dheeraj Varghese. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>